{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing\n",
    "methods:Tokenization, POS Tagging, stop words removal, Stemming andLemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and InverseDocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Dr. Piyush is learning NLP. It is very interesting and exciting. It is an important area of AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements. The tokens become the input for another process like parsing and text mining. Tokenization is useful because it breaks the text into smaller, more manageable parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Piyush is learning NLP.', 'It is very interesting and exciting.', 'It is an important area of AI.']\n",
      "Dr.\n",
      "Piyush\n",
      "is\n",
      "learning\n",
      "NLP\n",
      ".\n",
      "It\n",
      "is\n",
      "very\n",
      "interesting\n",
      "and\n",
      "exciting\n",
      ".\n",
      "It\n",
      "is\n",
      "an\n",
      "important\n",
      "area\n",
      "of\n",
      "AI\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.sent_tokenize(text) # Sentence Tokenization used to split the text into sentences\n",
    "print(sentences) \n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "Part-of-speech tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context. Part-of-speech tagging also known as word classes or lexical categories. The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Piyush', 'is', 'learning', 'NLP', '.', 'It', 'is', 'very', 'interesting', 'and', 'exciting', '.', 'It', 'is', 'an', 'important', 'area', 'of', 'AI', '.']\n",
      "[('Dr.', 'NNP'), ('Piyush', 'NNP'), ('is', 'VBZ'), ('learning', 'VBG'), ('NLP', 'NNP'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('very', 'RB'), ('interesting', 'JJ'), ('and', 'CC'), ('exciting', 'VBG'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('an', 'DT'), ('important', 'JJ'), ('area', 'NN'), ('of', 'IN'), ('AI', 'NNP'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words Removal\n",
    "Stop words are the most common words in a language like “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at”, etc. Stop words are removed to improve the performance of the model. Stop words are removed to reduce the dimensionality of the data and remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most', \"weren't\", 'should', 'them', 'when', 'no', \"you're\", 'haven', 'do', 'did', 'my', 'too', 'in', 'hasn', \"didn't\", 'up', 'for', 'hadn', 'won', 'yourself', 'they', 'were', 'a', 'your', 'll', 'yourselves', 'had', \"it's\", 'having', 'few', 's', 're', \"that'll\", 'both', 'have', 'just', \"shouldn't\", 'until', 'm', 'further', 'himself', 'where', 'o', 'aren', 'so', 't', 'because', 'theirs', 'from', 'weren', \"mustn't\", 'wouldn', 'these', 'are', \"you've\", 'during', 'him', 'mustn', 'very', 'will', 'whom', 'before', 'once', \"hasn't\", 'through', 'all', \"doesn't\", \"haven't\", 'he', 'at', 'wasn', 'of', 'doing', 'down', 'other', 'out', 'ain', \"isn't\", 'needn', 'on', 'being', 'to', 'has', 'i', 'doesn', \"you'll\", 'some', 'an', 'above', 'which', 'you', 'be', 'itself', 'about', \"won't\", 'who', 'with', \"couldn't\", 'not', 'hers', 'yours', 'is', 'more', 'than', 'herself', 'y', 'our', 'didn', \"wasn't\", 'as', \"shan't\", 'over', 'and', 'd', 'themselves', 'below', \"needn't\", 'their', 'why', 'only', 'there', 've', 'was', 'same', 'what', 'now', 'myself', \"aren't\", 'ma', 'ourselves', 'isn', 'her', 'under', 'it', 'then', \"hadn't\", 'me', \"you'd\", 'here', 'such', 'can', 'any', 'does', 'how', \"she's\", 'am', 'those', 'shouldn', 'each', 'into', 'that', 'mightn', 'don', \"mightn't\", 'or', 'if', 'been', 'after', 'his', \"wouldn't\", 'while', 'nor', 'the', 'between', 'she', 'off', 'shan', \"should've\", 'but', 'we', 'its', 'this', 'again', 'against', 'by', 'own', 'couldn', \"don't\", 'ours'}\n",
      "['Dr.', 'Piyush', 'is', 'learning', 'NLP', '.', 'It', 'is', 'very', 'interesting', 'and', 'exciting', '.', 'It', 'is', 'an', 'important', 'area', 'of', 'AI', '.']\n",
      "['Dr.', 'Piyush', 'learning', 'NLP', '.', 'interesting', 'exciting', '.', 'important', 'area', 'AI', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stop words removal\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "filtered_words = []\n",
    "for word in words:\n",
    "    if word.lower() not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Example: The stem of the word working => work. The stem of the word worked => work. The stem of the word works => work. \n",
    "\n",
    "It just removes the suffixes from the word and reduces it to its root word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Piyush', 'is', 'learning', 'NLP', '.', 'It', 'is', 'very', 'interesting', 'and', 'exciting', '.', 'It', 'is', 'an', 'important', 'area', 'of', 'AI', '.']\n",
      "['dr.', 'piyush', 'is', 'learn', 'nlp', '.', 'it', 'is', 'veri', 'interest', 'and', 'excit', '.', 'it', 'is', 'an', 'import', 'area', 'of', 'ai', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in words:\n",
    "    stemmed_words.append(ps.stem(word))\n",
    "    \n",
    "print(stemmed_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
