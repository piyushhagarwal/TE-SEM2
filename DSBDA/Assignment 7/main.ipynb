{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing\n",
    "methods:Tokenization, POS Tagging, stop words removal, Stemming andLemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and InverseDocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Dr. Piyush is learning NLP. It is very interesting and exciting. It is an important area of AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements. The tokens become the input for another process like parsing and text mining. Tokenization is useful because it breaks the text into smaller, more manageable parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Piyush is learning NLP.', 'It is very interesting and exciting.', 'It is an important area of AI.']\n",
      "Dr.\n",
      "Piyush\n",
      "is\n",
      "learning\n",
      "NLP\n",
      ".\n",
      "It\n",
      "is\n",
      "very\n",
      "interesting\n",
      "and\n",
      "exciting\n",
      ".\n",
      "It\n",
      "is\n",
      "an\n",
      "important\n",
      "area\n",
      "of\n",
      "AI\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "\n",
    "sentences = nltk.sent_tokenize(text) # Sentence Tokenization used to split the text into sentences\n",
    "print(sentences) \n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        print(word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
